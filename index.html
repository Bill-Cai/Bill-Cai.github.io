<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>My Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="My Blog">
<meta property="og:url" content="http://bill-cai.github.io/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Bill Cai">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="My Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/blog.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">My Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://bill-cai.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Spark环境配置-单机Local模式" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/10/Spark%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/" class="article-date">
  <time class="dt-published" datetime="2023-10-10T02:51:32.000Z" itemprop="datePublished">2023-10-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/10/Spark%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/">Spark环境配置-单机Local模式</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />

<h2 id="Local模式基本原理"><a href="#Local模式基本原理" class="headerlink" title="Local模式基本原理"></a>Local模式基本原理</h2><p>本质：启动一个JVM Process进程（<strong>一个进程内有多个线程</strong>，多线程的并行化），执行任务（Task）。</p>
<p>Local模式可以限制模拟Spark集群环境的线程数量，即<code>Local[N]</code>或<code>Local[*]</code>。</p>
<ul>
<li>N表示可以使用N个线程（不指定N，则默认1个线程），每个线程拥有一个CPU Core。通常CPU有几个Core就指定几个线程。</li>
<li><code>Local[*]</code>表示按照CPU最多的Cores设置线程数。</li>
</ul>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231010105503.png" alt="Local模式：一个进程内有多个线程"></p>
<p>Local模式下，有一个Spark程序开一个Local进程</p>
<ul>
<li>一个Local进程只能运行一个Spark程序；</li>
<li>如果要运行多个Spark程序，则需要执行多个相互独立的Local进程。</li>
</ul>
<p>每个Local进程中：</p>
<ul>
<li>Master和Worker都是Local进程本身</li>
<li>Driver也是Local本身，没有独立的Executor角色</li>
<li>由Local进程（Driver）的线程提供计算能力</li>
</ul>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231010105920.png" alt="多任务，多线程"></p>
<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>按照<a href="/2023/10/09/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/" title="Hadoop环境配置-单机Local模式">Hadoop环境配置-单机Local模式</a>配置hadoop单机环境。即，完成JDK和hadoop的安装配置。</p>
<h3 id="安装conda"><a href="#安装conda" class="headerlink" title="安装conda"></a>安装conda</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda</span><br></pre></td></tr></table></figure>

<p><strong>添加环境变量</strong>（<code>vim /etc/profile</code>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/export/server/conda/bin</span><br></pre></td></tr></table></figure>

<p>并激活（<code>source /etc/profile</code>）。</p>
<p><strong>初始化conda</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">conda init bash</span><br><span class="line">conda config --<span class="built_in">set</span> auto_update_conda off &amp;&amp; \</span><br><span class="line">    conda config --<span class="built_in">set</span> auto_activate_base off &amp;&amp; \</span><br><span class="line">    conda config --add channels conda-forge &amp;&amp; \</span><br><span class="line">    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 &amp;&amp; \</span><br><span class="line">    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro &amp;&amp; \</span><br><span class="line">    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r &amp;&amp; \</span><br><span class="line">    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free &amp;&amp; \</span><br><span class="line">    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main &amp;&amp; \</span><br><span class="line">    conda config --<span class="built_in">set</span> channel_priority flexible</span><br></pre></td></tr></table></figure>

<h3 id="创建Python虚拟环境"><a href="#创建Python虚拟环境" class="headerlink" title="创建Python虚拟环境"></a>创建Python虚拟环境</h3><p>推荐使用Python:3.8的解释器环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 -y</span><br></pre></td></tr></table></figure>

<p>激活环境：<code>conda activate pyspark</code></p>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><p>Spark下载地址：</p>
<ul>
<li>官网：<a target="_blank" rel="noopener" href="https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz">https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz</a></li>
</ul>
<p>下载并解压Spark，创建软链接：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line">curl -LO https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz</span><br><span class="line">tar -zxvf spark-3.5.0-bin-hadoop3.tgz -C /export/server</span><br><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.5.0-bin-hadoop3 /export/server/spark</span><br></pre></td></tr></table></figure>

<p><strong>添加环境变量</strong>（<code>vim /etc/profile</code>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/server/spark</span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/conda/envs/pyspark/bin/python</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>并激活（<code>source /etc/profile</code>）。</p>
<h2 id="运行Local模式"><a href="#运行Local模式" class="headerlink" title="运行Local模式"></a>运行Local模式</h2><p>因为已经添加了<code>PYSPARK_PYTHON</code>，可以直接运行该文件夹下的<code>pyspark</code>等程序。</p>
<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master <span class="built_in">local</span>[*]</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231010111939.png" alt="运行spark程序"></p>
<h3 id="pyspark样例"><a href="#pyspark样例" class="headerlink" title="pyspark样例"></a>pyspark样例</h3><p>运行Spark的Python样例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master <span class="built_in">local</span>[*] /export/server/spark/examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure>

<p><code>pi.py</code>为Python的Spark并行计算程序，代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">from random import random</span><br><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        Usage: pi [partitions]</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;PythonPi&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    partitions = int(sys.argv[1]) <span class="keyword">if</span> len(sys.argv) &gt; 1 <span class="keyword">else</span> 2</span><br><span class="line">    n = 100000 * partitions</span><br><span class="line"></span><br><span class="line">    def f(_: int) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        x = random() * 2 - 1</span><br><span class="line">        y = random() * 2 - 1</span><br><span class="line">        <span class="built_in">return</span> 1 <span class="keyword">if</span> x ** 2 + y ** 2 &lt;= 1 <span class="keyword">else</span> 0</span><br><span class="line"></span><br><span class="line">    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Pi is roughly %f&quot;</span> % (4.0 * count / n))</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bill-cai.github.io/2023/10/10/Spark%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/" data-id="clnjr9kld0014uculbbebhyc9" data-title="Spark环境配置-单机Local模式" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Hadoop环境配置-单机Local模式" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/09/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/" class="article-date">
  <time class="dt-published" datetime="2023-10-09T13:10:11.000Z" itemprop="datePublished">2023-10-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/09/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/">Hadoop环境配置-单机Local模式</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />

<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><h3 id="Ubuntu单机配置"><a href="#Ubuntu单机配置" class="headerlink" title="Ubuntu单机配置"></a>Ubuntu单机配置</h3><p>基于Linux单机（Ubuntu:22.04）安装hadoop，以<strong>运行local模式</strong>。</p>
<p>安装<strong>基本应用</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install -y vim</span><br><span class="line">apt-get install -y openssh-server</span><br><span class="line">apt-get install -y curl</span><br></pre></td></tr></table></figure>

<p>安装<strong>OpenJDK</strong>（选择JDK-1.8）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y openjdk-8-jdk</span><br></pre></td></tr></table></figure>

<p>则安装的jdk在：<code>/usr/lib/jvm/java-8-openjdk-amd64</code></p>
<p>修改配置（<code>vim /etc/profile</code>），增加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>激活（<code>source /etc/profile</code>）后检测：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">javac -version</span><br></pre></td></tr></table></figure>

<h3 id="root用户和hadoop用户免密互相登录"><a href="#root用户和hadoop用户免密互相登录" class="headerlink" title="root用户和hadoop用户免密互相登录"></a>root用户和hadoop用户免密互相登录</h3><p>参考<a href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" title="Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）">Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）</a>，配置单机的<strong>root用户</strong>和<strong>hadoop用户</strong>免密互相登录。</p>
<p>过程如下：</p>
<ol>
<li><p>分别以root用户和hadoop用户登录，生成密钥对。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
</li>
<li><p>分别拷贝SSH密钥</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@localhost</span><br><span class="line">ssh-copy-id hadoop@localhost</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查<code>~/.ssh/authorized_keys</code>中是否已经添加了root和hadoop的rsa凭证。</p>
</li>
</ol>
<p>如果没有配置免密登录，则后续运行<code>start-dfs.sh</code>可能出现报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Permission denied (publickey,password).</span><br></pre></td></tr></table></figure>

<h2 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h2><p>下载解压hadoop：</p>
<ul>
<li>官网下载地址：<a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz">https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</a></li>
<li>清华镜像源：<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz">https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /export/server</span><br><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line">curl -LO https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</span><br><span class="line">tar -zxvf hadoop-3.3.6.tar.gz -C /export/server</span><br></pre></td></tr></table></figure>

<p>创建软链接（其实重命名文件夹也行）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/hadoop-3.3.6 hadoop</span><br></pre></td></tr></table></figure>

<h2 id="配置hadoop单机模式"><a href="#配置hadoop单机模式" class="headerlink" title="配置hadoop单机模式"></a>配置hadoop单机模式</h2><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>修改<code>vim /etc/profile</code>，增加hadoop的<code>bin</code>和<code>sbin</code>路径：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>

<p>激活（<code>source /etc/profile</code>）后检测：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop</span><br></pre></td></tr></table></figure>

<h3 id="配置workers文件"><a href="#配置workers文件" class="headerlink" title="配置workers文件"></a>配置<code>workers</code>文件</h3><p>修改配置（<code>vim /export/server/hadoop/etc/hadoop/workers</code>）文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>

<p>表示工作节点只有本机。</p>
<h3 id="配置hadoop-env-sh文件"><a href="#配置hadoop-env-sh文件" class="headerlink" title="配置hadoop-env.sh文件"></a>配置<code>hadoop-env.sh</code>文件</h3><p>在文件开头增加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_LOG_DIR=<span class="variable">$HADOOP_HOME</span>/logs</span><br></pre></td></tr></table></figure>

<p>这里的<code>$HADOOP_HOME/logs</code>在后续运行中会自动生成。</p>
<h3 id="配置core-site-xml文件"><a href="#配置core-site-xml文件" class="headerlink" title="配置core-site.xml文件"></a>配置<code>core-site.xml</code>文件</h3><p>在标签<code>&lt;configuration&gt;</code>中设置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="配置hdfs-site-xml文件"><a href="#配置hdfs-site-xml文件" class="headerlink" title="配置hdfs-site.xml文件"></a>配置<code>hdfs-site.xml</code>文件</h3><p>在标签<code>&lt;configuration&gt;</code>中设置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>700<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>268435456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/dn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>生成对应的文件夹：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /data/nn</span><br><span class="line"><span class="built_in">mkdir</span> -p /data/dn</span><br></pre></td></tr></table></figure>

<h2 id="运行hadoop的Local模式"><a href="#运行hadoop的Local模式" class="headerlink" title="运行hadoop的Local模式"></a>运行hadoop的Local模式</h2><h3 id="授权hadoop用户"><a href="#授权hadoop用户" class="headerlink" title="授权hadoop用户"></a>授权hadoop用户</h3><p>hadoop默认不允许root用户运行，<strong>需要将hadoop相关文件夹授权给hadoop用户</strong>（普通用户）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chown</span> -R hadoop:hadoop /export</span><br><span class="line"><span class="built_in">chown</span> -R hadoop:hadoop /data</span><br></pre></td></tr></table></figure>

<p>切换用户：<code>su - hadoop</code>或者<code>ssh hadoop@localhost</code></p>
<h3 id="初始化hadoop系统"><a href="#初始化hadoop系统" class="headerlink" title="初始化hadoop系统"></a>初始化hadoop系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<h3 id="启动HDFS集群"><a href="#启动HDFS集群" class="headerlink" title="启动HDFS集群"></a>启动HDFS集群</h3><p>一键启动hdfs集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>查看节点组成（可以在各个节点上看）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>一键关闭hdfs集群：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>单机运行hadoop的Local模式，则所有的节点均在本机上：</p>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231009215604.png" alt="配一张成功的图"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bill-cai.github.io/2023/10/09/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/" data-id="clnjr9kl6000ducul8tordw0q" data-title="Hadoop环境配置-单机Local模式" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Linux集群配置hadoop用户SSH免密登录" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/06/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E7%94%A8%E6%88%B7SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" class="article-date">
  <time class="dt-published" datetime="2023-10-06T07:13:49.000Z" itemprop="datePublished">2023-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/06/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E7%94%A8%E6%88%B7SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/">Linux集群配置hadoop用户SSH免密登录</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />

<h2 id="配置hadoop用户SSH免密登录"><a href="#配置hadoop用户SSH免密登录" class="headerlink" title="配置hadoop用户SSH免密登录"></a>配置hadoop用户SSH免密登录</h2><p>上一篇<a href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" title="Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）">Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）</a>记录了Linux集群间<strong>root用户</strong>怎么配置SSH免密互相登录。</p>
<p>本文记录hadoop用户（<strong>普通用户</strong>）配置SSH免密互相登录的过程。原理基本相同。</p>
<p>以下操作没有特别说明，即在集群<strong>所有Linux系统</strong>上进行的操作。</p>
<h3 id="添加普通用户"><a href="#添加普通用户" class="headerlink" title="添加普通用户"></a>添加普通用户</h3><p>使用root用户登录，通过<code>useradd</code>添加普通用户</p>
<ul>
<li><code>-m</code>，表示自动生成用户目录，即<code>/home/&lt;username&gt;</code></li>
<li><code>-s</code>，用于指定用户的终端程序（默认是sh终端，很垃圾，建议换成bash）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -m -s /bin/bash hadoop</span><br></pre></td></tr></table></figure>

<p>这里建议所有机子上的用户名称一致（后面<code>ssh &lt;host&gt;</code>的操作就会很丝滑），不然就得使用<code>ssh &lt;username&gt;@&lt;host&gt;</code>或者再去配置用户名映射。</p>
<p>设置用户密码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure>

<h3 id="修改SSH配置"><a href="#修改SSH配置" class="headerlink" title="修改SSH配置"></a>修改SSH配置</h3><p>通过<code>vim /etc/ssh/sshd_config</code>命令修改SSH配置文件，找到并更改以下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UsePAM no</span><br></pre></td></tr></table></figure>

<p>这是为了避免在Docker容器中使用SSH进行普通用户登录时，出现登录后链接马上断开的问题（<code>Connection to &lt;host&gt; closed</code>）。</p>
<p>重启SSH服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service ssh restart</span><br></pre></td></tr></table></figure>

<h3 id="普通用户配置"><a href="#普通用户配置" class="headerlink" title="普通用户配置"></a>普通用户配置</h3><p>切换到hadoop用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br></pre></td></tr></table></figure>

<p>生成SSH密钥对</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>分配公钥（具体说明见<a href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" title="Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）">Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）</a>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id hadoop@node1</span><br><span class="line">ssh-copy-id hadoop@node2</span><br><span class="line">ssh-copy-id hadoop@node3</span><br></pre></td></tr></table></figure>

<p>查看<code>~/.ssh/authorized_keys</code>中是否已经添加了3台机子的rsa凭证。（如果没有，可以手动生成该文件并copy文件内容）</p>
<p>可以通过<code>ssh</code>免密登录了：</p>
<ul>
<li>使用<code>ssh hadoop@node1</code></li>
<li>也可以直接<code>ssh node1</code>，因为用户名一致</li>
</ul>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231006154335.png" alt="配一张成功的图"></p>
<h2 id="总结经验"><a href="#总结经验" class="headerlink" title="总结经验"></a>总结经验</h2><p>主要就是两方面的配置：SSH配置文件的更改，密钥的正确使用。</p>
<ul>
<li>配置文件需要保证：允许root用户登录，允许公钥登录，禁用<code>UsePAM</code></li>
<li>每台机子都要有其他各台机子的公钥，并加入<code>~/.ssh/authorized_keys</code>认证才能免密登录其他各台机子</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43790540/article/details/104848610">阿里云Ubuntu下创建新用户并且配置bash_阿里云服务器 ubuntu 添加新用户-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_30872867/article/details/96985340#:~:text=%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E5%A6%82%E4%B8%8B%EF%BC%9Asudo%20ssh-keygen%20-t%20dsa%20-f,%2Fetc%2Fssh%2Fssh_host_dsa_key%20%28%E4%B8%80%E7%9B%B4%E5%9B%9E%E8%BD%A6%29sudo%20ssh-keyg..._connection%20to%20closed">解决 Docker Hadoop ssh “Connection to * closed”.问题-CSDN博客</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bill-cai.github.io/2023/10/06/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E7%94%A8%E6%88%B7SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" data-id="clnjr9kl8000kucul4muv99sn" data-title="Linux集群配置hadoop用户SSH免密登录" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Linux集群配置SSH免密登录" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" class="article-date">
  <time class="dt-published" datetime="2023-10-05T11:48:51.000Z" itemprop="datePublished">2023-10-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/">Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />

<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在学习大数据和高性能计算时，需要搭建Hadoop集群环境。</p>
<p>Hadoop集群采取中心化模式的分布式框架，即采用一个中心节点（服务器）统筹其他服务器的工作模式。</p>
<p>网上教程大多采用多台虚拟机或多台云服务器进行搭建，可以参考<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1WY4y197g7">2023新版黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽_哔哩哔哩_bilibili</a>。</p>
<p>如果使用虚拟机，基本的配置步骤较多，大概为：</p>
<ol>
<li>配置固定IP地址</li>
<li>配置主机名映射</li>
<li>配置SSH免密登录</li>
<li>创建普通用户并配置SSH免密登录</li>
<li>关闭防火墙、SELinux</li>
<li>时间同步</li>
<li>快照保存</li>
</ol>
<p>使用云服务器（以阿里云ECS为例），基本的配置步骤有：</p>
<ol>
<li>配置主机名映射</li>
<li>配置SSH免密登录</li>
<li>创建普通用户并配置SSH免密登录</li>
</ol>
<p>固定IP、同步时间、防火墙等在云服务器中不需要进行配置，就比较简洁。</p>
<p>运行虚拟机需要较高的计算机性能，多台云服务器又有点费钱；故，尝试采用<a target="_blank" rel="noopener" href="https://www.aliyun.com/product/kubernetes">阿里云容器服务Kubernetes版（ACK）</a>，基于多个容器构建Hadoop集群。</p>
<p>以容器进行集群搭建，和基于云服务器的步骤差不多。</p>
<p>本文主要介绍ACK容器部署和容器的基本配置。重点说明基于<strong>ubuntu:22.04</strong>镜像运行的容器如何配置<strong>主机名映射</strong>和<strong>SSH免密登录</strong>，为后续安装Hadoop做准备。</p>
<h2 id="环境基本介绍"><a href="#环境基本介绍" class="headerlink" title="环境基本介绍"></a>环境基本介绍</h2><p>基于镜像<code>ubuntu:22.04</code>运行3个容器，开放<code>22</code>端口作为SSH的默认端口。</p>
<ul>
<li>当然也可以在服务中配置不同的端口映射，这里为了简单直接将外部<code>22</code>端口映射容器<code>22</code>端口</li>
<li>否则，在容器内用SSH连接其他容器时需要使用<code>-p &lt;port&gt;</code>，例如，<code>ssh-copy-id -p &lt;port&gt; node1</code></li>
</ul>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231005201837.png" alt="使用镜像创建应用"></p>
<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231005202349.png" alt="创建端口映射"></p>
<p>容器的IP地址和端口即是容器服务的外部端点。</p>
<h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><h3 id="1-安装基本应用"><a href="#1-安装基本应用" class="headerlink" title="1 安装基本应用"></a>1 安装基本应用</h3><p>这里的每个容器都相当于一台独立的Linux服务器，先给每个容器安装基本的应用（因为ubuntu基本啥都没有），包括Vim编辑器、openssh-server（SSH连接）、inetutils-ping（测试能否<code>ping</code>通）。</p>
<ul>
<li>这里外部暂时无法连接容器，需要在阿里云的工作台开启容器终端进行操作。</li>
<li>后续操作没有特别说明，默认是<strong>对每个容器进行相同的操作</strong>（虽然后面安装Hadoop时节点分主从，但是本文中的操作，三个节点是等同的）。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install -y vim</span><br><span class="line">apt-get install -y openssh-server</span><br><span class="line">apt-get install -y inetutils-ping</span><br></pre></td></tr></table></figure>

<h3 id="2-设置root密码"><a href="#2-设置root密码" class="headerlink" title="2 设置root密码"></a>2 设置root密码</h3><p>通过</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>

<p>设置root密码（相当于虚拟机或云服务器的登录密码），建议保持一致，免得忘记。</p>
<h3 id="3-配置主机名映射"><a href="#3-配置主机名映射" class="headerlink" title="3 配置主机名映射"></a>3 配置主机名映射</h3><p>这里，需要稍微了解一点主机IP和域名解析的知识：<a href="/2023/10/04/IP%E5%92%8C%E4%B8%BB%E6%9C%BA%E5%90%8D/" title="Linux IP和主机名">Linux IP和主机名</a></p>
<p>Linux操作系统解析主机名的文件一般在<code>/etc/hosts</code>，故编辑该文件（<code>vim /etc/hosts</code>），在其末尾加上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xxx.xxx.xxx.xx1    ubuntu-node1-xxx-xxx    node1</span><br><span class="line">xxx.xxx.xxx.xx2    ubuntu-node2-xxx-xxx    node2</span><br><span class="line">xxx.xxx.xxx.xx3    ubuntu-node3-xxx-xxx    node3</span><br></pre></td></tr></table></figure>

<p>这里第一列为三个容器的IP地址，第二列为容器名（主机名），第三列为主机名缩写（自定义）。中间用制表符分隔。</p>
<ul>
<li>这里相当于把3个IP和主机名做了个映射，这样Linux操作系统才能识别出<code>node1</code>的IP是<code>xxx.xxx.xxx.xx1</code>。</li>
</ul>
<p>更改完成后，检查一下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/hosts</span><br></pre></td></tr></table></figure>

<p>不要出问题。</p>
<h3 id="配置SSH免密登录"><a href="#配置SSH免密登录" class="headerlink" title="配置SSH免密登录"></a>配置SSH免密登录</h3><p>修改SSH的配置文件<code>/etc/ssh/sshd_config</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>

<p>更改以下配置（可以在配置文件的末尾直接增加以下内容）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PermitRootLogin <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br><span class="line">PasswordAuthentication <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>

<p>做一些说明：</p>
<ul>
<li><code>PermitRootLogin yes</code>，是允许<strong>远程</strong>以<strong>root用户</strong>进行登录</li>
<li><code>PubkeyAuthentication yes</code>，是允许使用<strong>公钥</strong>进行登录</li>
<li><code>PasswordAuthentication yes</code>，是允许使用<strong>密码</strong>进行登录（后期可以禁用，改为<code>PasswordAuthentication no</code>）</li>
<li>（也可以增加<code>RSAAuthentication yes</code>配置，允许RSA认证。虽然影响不大，好像这东西已经被弃用了。）</li>
</ul>
<p>检查配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>

<p>重启SSH服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service ssh restart</span><br></pre></td></tr></table></figure>

<h3 id="实现相互SSH免密登录"><a href="#实现相互SSH免密登录" class="headerlink" title="实现相互SSH免密登录"></a>实现相互SSH免密登录</h3><p>在每个容器内，<strong>生成密钥对</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>生成过程中直接回车（默认路径、不使用密码验证），生成的密钥对（即，公钥<code>id_rsa.pub</code>和私钥<code>id_rsa</code>）存放在默认路径<code>~/.ssh</code>下。</p>
<p><strong>将每个容器的公钥复制给所有容器（包括自身）</strong></p>
<ul>
<li>因为自身的公钥也需要注册到<code>~/.ssh/authorized_keys</code>文件中，才能SSH访问自身</li>
<li>复制需要输入<strong>目标容器</strong>的密码</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node1</span><br><span class="line">ssh-copy-id node2</span><br><span class="line">ssh-copy-id node3</span><br></pre></td></tr></table></figure>

<p><strong>这里成功的依据是，<code>~/.ssh/authorized_keys</code>中已经添加了3台机子的rsa凭证。</strong></p>
<p>直接使用SSH连接容器</p>
<ul>
<li>第一次连接需要输入<code>yes</code>确认凭证，可能需要输入密码（不确定，我试的时候一开始免密设置没成功所以需要密码）。</li>
<li>这里第一次连接，会在<code>~/.ssh</code>文件夹下生成<code>known_hosts</code>文件，保存已知的连接主机。</li>
<li>后续，直接<code>ssh</code>就不需要密码了！！！</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh node1</span><br><span class="line">ssh node2</span><br><span class="line">ssh node3</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/bill-cai-1020/md-picgo/raw/master/pic/20231005211002.png" alt="配一张成功的图"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010993514/article/details/82083027">Linux系统配置SSH免密登录(多主机互通)_.ssh 地下没有 au-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/9dbb1dea5929">记一次ssh免密登录踩坑and Debug之路</a></p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/810262">Linux: SSH免密登录配置完了不生效-阿里云开发者社区</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhangmingcai/article/details/95734889">ssh免密登陆失败原因总结(Linux)_ssh免密不成功-CSDN博客</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bill-cai.github.io/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/" data-id="clnjr9kl9000nuculbgxubnbm" data-title="Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Linux环境变量" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/04/Linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/" class="article-date">
  <time class="dt-published" datetime="2023-10-04T04:56:30.000Z" itemprop="datePublished">2023-10-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/04/Linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/">Linux环境变量</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>Linux上使用的一系列命令本质上是一个个的可执行程序。</p>
<ul>
<li>例如，<code>bash</code>本质上是<code>/bin/bash</code></li>
</ul>
<p>之所以这些命令无论在什么工作目录都能执行，就是因为有<strong>环境变量</strong>。</p>
<p>环境变量，是操作系统在运行时记录的一些关键性的信息，用以辅助系统运行。</p>
<p>在Linux系统中通过<code>env</code>命令可以查看当前系统中记录的环境变量。</p>
<ul>
<li>环境变量以<code>key=value</code>的形式记录。</li>
</ul>
<h2 id="PATH"><a href="#PATH" class="headerlink" title="PATH"></a>PATH</h2><p>PATH记录了系统执行任何命令的搜索路径。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">env</span> | grep PATH</span><br><span class="line">PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib</span><br></pre></td></tr></table></figure>

<p>当执行命令时，会按照上面的顺序搜索要执行的程序的本体。</p>
<h2 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h2><h3 id="符号"><a href="#符号" class="headerlink" title="$符号"></a><code>$</code>符号</h3><p><code>$</code>符号用于取变量的值。</p>
<ul>
<li>可以通过<code>&#123;&#125;</code>标注要取的变量</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib</span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$&#123;PATH&#125;</span>ABC</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/libABC</span><br></pre></td></tr></table></figure>

<h3 id="临时设置"><a href="#临时设置" class="headerlink" title="临时设置"></a>临时设置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> 变量名=变量值</span><br></pre></td></tr></table></figure>

<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;/opt/conda/bin:<span class="variable">$&#123;PATH&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="永久生效"><a href="#永久生效" class="headerlink" title="永久生效"></a>永久生效</h3><p>针对当前用户，修改配置文件<code>~/.bashrc</code></p>
<p>针对所有用户，修改配置文件<code>/etc/profile</code></p>
<p>执行<code>source 配置文件</code>使修改立即生效（或者重新登录终端）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bill-cai.github.io/2023/10/04/Linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/" data-id="clnjr9kla000pucul8ozm3vkr" data-title="Linux环境变量" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cluster-analysis/" rel="tag">Cluster analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Utils/" rel="tag">Utils</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Cluster-analysis/" style="font-size: 10px;">Cluster analysis</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Linux/" style="font-size: 20px;">Linux</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/Utils/" style="font-size: 10px;">Utils</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/10/10/Spark%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/">Spark环境配置-单机Local模式</a>
          </li>
        
          <li>
            <a href="/2023/10/09/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%8D%95%E6%9C%BALocal%E6%A8%A1%E5%BC%8F/">Hadoop环境配置-单机Local模式</a>
          </li>
        
          <li>
            <a href="/2023/10/06/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E7%94%A8%E6%88%B7SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/">Linux集群配置hadoop用户SSH免密登录</a>
          </li>
        
          <li>
            <a href="/2023/10/05/Linux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/">Linux集群配置SSH免密登录 - 基于阿里云容器服务Kubernetes版（ACK）</a>
          </li>
        
          <li>
            <a href="/2023/10/04/Linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/">Linux环境变量</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Bill Cai<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






  </div>
</body>
</html>